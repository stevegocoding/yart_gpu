
#include "kernel_data.h"
#include "kdtree_kernel_data.h"
#include "cuda_utils.h"
#include "cuda_utils_device.h"
#include "functor_device.h"

// Defined in kdtree.cu
extern cudaDeviceProp device_props;

// ---------------------------------------------------------------------
/*
/// \brief	Computes chunk count for each node.
///
///			The chunk count is defined as the number of chunks required for a given node. It is
///			computed by taking into account that a chunk may only contain up to ::KD_CHUNKSIZE
///			elements.
*/ 
// ---------------------------------------------------------------------
__global__ void kernel_get_chunk_counts(uint32 *d_num_elems_array, uint32 num_nodes, uint32 *d_out_chunk_counts)
{
	uint32 idx = blockIdx.x * blockDim.x + threadIdx.x;
	
	if (idx < num_nodes)
	{
		uint32 num_elems = d_num_elems_array[idx];

		// NOTE: These division and modulo operation are optimized by the compiler since
		//		 KD_CHUNKSIZE is power of 2.
		d_out_chunk_counts[idx] = num_elems / KD_CHUNKSIZE + ((num_elems % KD_CHUNKSIZE)? 1 : 0);
	}
}

// ---------------------------------------------------------------------
/*
/// \brief	Generates chunks for kd-tree nodes. 
///
///			Chunks are generated by iterative creation within node borders. All nodes are handled
///			in parallel. Provided offsets permit this approach.
///
/// \warning Heavy uncoalesced memory access. 
/// \note	Remember to set chunk count. This cannot be done within this kernel. 
*/ 
// ---------------------------------------------------------------------
__global__ void kernel_gen_chunks(uint32 *d_num_elems_array, uint32 *d_first_elem_idx_array, uint32 num_nodes, 
								uint32 *d_offsets_array, c_kd_chunk_list chunk_list)
{
	// 
	uint32 node_idx = blockIdx.x * blockDim.x + threadIdx.x;
	
	if (node_idx < num_nodes)
	{
		uint32 idx_start = d_offsets_array[node_idx];
		uint32 first_elem = d_first_elem_idx_array[node_idx]; 
		uint32 num_elems = d_num_elems_array[node_idx];

		uint32 cur_tri = 0, i = 0; 
		
		while (cur_tri < num_elems)
		{
			/// Generate chunks
			chunk_list.d_node_idx[idx_start+i] = node_idx; 
			chunk_list.d_first_elem_idx[idx_start+i] = first_elem + cur_tri;
			
			uint32 num_tris_chunk = min(KD_CHUNKSIZE, num_elems-cur_tri);
			chunk_list.d_num_elems[idx_start+i] = num_tris_chunk;
			cur_tri += num_tris_chunk;
			++i; 
		}
	}
}

__global__ void kernel_count_elems_chunk(c_kd_chunk_list chunks_list, uint32 *d_valid_flags, uint32 *d_out_num_elems)
{
	uint32 chk = CUDA_GRID2DINDEX;

	__shared__ uint32 s_num_elems_chunk;
	__shared__ uint32 s_idx_first_elem;
	
	if (threadIdx.x == 0)
	{
		s_num_elems_chunk = chunks_list.d_num_elems[chk];
		s_idx_first_elem = chunks_list.d_first_elem_idx[chk];
	}

	__syncthreads();
	
	// Copy chunks's flags into shared memory. Preload and -add two values directly.
	__shared__ uint32 s_mem[KD_CHUNKSIZE];
	uint32 v1 = 0; 
	uint32 v2 = 0;
	
	if (threadIdx.x < s_num_elems_chunk)
		v1 = d_valid_flags[s_idx_first_elem + threadIdx.x];
	if (threadIdx.x + blockDim.x < s_num_elems_chunk)
		v2 = d_valid_flags[s_idx_first_elem + threadIdx.x + blockDim.x];
	s_mem[threadIdx.x] = v1 + v2; 
	__syncthreads();

	// Now perform reduction on chunks's flags.
	uint32 res = device_reduce_fast<uint32, KD_CHUNKSIZE, op_add<uint32>>(s_mem);
	
	if (threadIdx.x == 0)
		d_out_num_elems[chk] = res; 
}

//////////////////////////////////////////////////////////////////////////

extern "C"
void kernel_wrapper_get_chunk_counts(uint32 *d_num_elems_node, uint32 num_nodes, uint32 *d_out_chunk_counts)
{
	dim3 block_size = dim3(256, 1, 1);
	dim3 grid_size = dim3(CUDA_DIVUP(num_nodes, block_size.x), 1, 1);
	kernel_get_chunk_counts<<<grid_size, block_size>>>(d_num_elems_node, num_nodes, d_out_chunk_counts); 

	CUDA_CHECKERROR;
}

extern "C"
void kernel_wrapper_kd_gen_chunks(uint32 *d_num_elems_array, 
								uint32 *d_idx_first_elem_array, 
								uint32 num_nodes, 
								uint32 *d_offsets, 
								c_kd_chunk_list& chunk_list)
{
	dim3 block_size = dim3(256, 1, 1);
	dim3 grid_size = dim3(CUDA_DIVUP(num_nodes, block_size.x), 1, 1);

	kernel_gen_chunks<<<grid_size, block_size>>>(d_num_elems_array, d_idx_first_elem_array, num_nodes, d_offsets, chunk_list);
}

extern "C"
void kernel_wrapper_count_elems_chunk(const c_kd_chunk_list& chunks_list, uint32 *d_valid_flags, uint32 *d_out_num_elems)
{
	// Note that we use half the chunk size here. This is a reduction optimization.
	dim3 block_size = dim3(KD_CHUNKSIZE/2, 1, 1);
	dim3 grid_size = CUDA_MAKEGRID2D(chunks_list.num_chunks, device_props.maxGridSize[0]);
	
	kernel_count_elems_chunk<<<grid_size, block_size>>>(chunks_list, d_valid_flags, d_out_num_elems);
}