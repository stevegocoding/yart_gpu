#include "kernel_data.h"
#include "kdtree_kernel_data.h" 
#include "cuda_utils_device.h"
#include "cuda_mem_pool.h"
#include "functor_device.h"
#include "cuda_rng.h"

cudaDeviceProp device_props;

__constant__ float k_empty_space_ratio; 


// ---------------------------------------------------------------------
/*
/// \brief	Slim version of KDNodeList for node AABB related tasks.
///
///			To avoid parameter space overflows.
*/ 
// ---------------------------------------------------------------------
struct kd_node_list_aabb
{
#ifdef __cplusplus
	kd_node_list_aabb(const c_kd_node_list& src)
		: num_nodes(src.num_nodes)
		, d_first_elem_idx(src.d_first_elem_idx)
		, d_num_elems(src.d_num_elems_array)
		, d_node_level(src.d_node_level)
		, d_aabb_tight_min(src.d_aabb_tight_min)
		, d_aabb_tight_max(src.d_aabb_tight_max)
		, d_aabb_inherit_min(src.d_aabb_inherit_min)
		, d_aabb_inherit_max(src.d_aabb_inherit_max)
		, d_elem_node_assocs(src.d_node_elems_list) 
	{

	}
#endif

	// Number of nodes in this list.
	uint32 num_nodes;

	// First element index address in ENA for each node (device memory).
	uint32 *d_first_elem_idx; 
	// Number of elements for each node (device memory).
	uint32 *d_num_elems;
	// Tight AABB minimum for each node (device memory).
	float4 *d_aabb_tight_min; 
	// Tight AABB maximum for each node (device memory).
	float4 *d_aabb_tight_max; 
	// Inherited AABB minimum for each node (device memory).
	float4 *d_aabb_inherit_min; 
	// Inherited AABB maximum for each node (device memory).
	float4 *d_aabb_inherit_max;
	// Node levels (device memory). Starting with 0 for root.
	uint32 *d_node_level;

	uint32 *d_elem_node_assocs;
	
};


// ---------------------------------------------------------------------
/*
/// \brief	Creates an empty leaf node in the final node list.
/// 		
/// 		Sets all required information for the empty node, that is node level, number of
/// 		elements, child addresses (both 0) and node bounding box. 
*/ 
// ---------------------------------------------------------------------
inline __device__ void device_create_empty_leaf(c_kd_final_node_list& final_list, uint32 idx_new, float aabb_min[3], float aabb_max[3], uint32 node_level)
{
	// d_idxFirstElem can stay undefined as we have no elements.
	final_list.d_num_elems[idx_new] = 0;
	final_list.d_node_level[idx_new] = node_level;
	// d_splitAxis, d_splitPos are undefined for leafs.
	final_list.d_child_left[idx_new] = 0;
	final_list.d_child_right[idx_new] = 0;
	final_list.d_aabb_min[idx_new] = make_float4(aabb_min[0], aabb_min[1], aabb_min[2], 0.f);
	final_list.d_aabb_max[idx_new] = make_float4(aabb_max[0], aabb_max[1], aabb_max[2], 0.f);
	// d_elemNodeAssoc: no changes required.	
}

// ---------------------------------------------------------------------
/*
/// \brief	Copies a final node list node.
/// 		
/// 		Used for empty space cutting to generate a copy of the actual node. This copy
/// 		represents the new, non-empty node. The empty node is generated using
/// 		dev_CreateEmptyLeaf(). 
*/ 
// ---------------------------------------------------------------------
inline __device__ void device_create_final_node_copy(c_kd_final_node_list& final_list, 
													uint32 idx_old, 
													uint32 idx_new, 
													float aabb_min[3], float aabb_max[3],
													uint32 node_level)
{
	// Both nodes use the same elements.
	final_list.d_first_elem_idx[idx_new] = final_list.d_first_elem_idx[idx_old];
	final_list.d_num_elems[idx_new] = final_list.d_num_elems[idx_old];
	final_list.d_node_level[idx_new] = node_level;
	// d_splitAxis, d_splitPos, d_childLeft, d_childRight are not yet known.
	final_list.d_aabb_min[idx_new] = make_float4(aabb_min[0], aabb_min[1], aabb_min[2], 0.f);
	final_list.d_aabb_max[idx_new] = make_float4(aabb_max[0], aabb_max[1], aabb_max[2], 0.f); 
}

inline __device__ void device_create_child(kd_node_list_aabb& next_list, uint32 idx_new, 
											float aabb_min[3], float aabb_max[3], uint32 node_level)
{
	next_list.d_node_level[idx_new] = node_level;
	next_list.d_aabb_inherit_max[idx_new] = make_float4(aabb_min[0], aabb_min[1], aabb_min[3], 0.0f);
	next_list.d_aabb_inherit_min[idx_new] = make_float4(aabb_max[0], aabb_max[1], aabb_max[3], 0.0f); 
}

// ---------------------------------------------------------------------
/*
/// \brief	Generates AABBs for chunks using parallel reduction.
/// 		
/// 		Chunk AABBs are generated by performing parallel reductions on the element AABBs
///			given in the node list.
///
/// \note	Required shared memory per thread block of size N: 8 * N bytes.
*/ 
// ---------------------------------------------------------------------
template <uint32 num_elems_pts>
__global__ void kernel_gen_chunk_aabb(c_kd_node_list node_list, c_kd_chunk_list chunk_list)
{
	uint32 chk = CUDA_GRID2DINDEX;

	__shared__ uint32 s_num_elems;
	__shared__ uint32 s_first_elem_idx; 
	if (threadIdx.x == 0)
	{
		s_num_elems = chunk_list.d_num_elems[chk];
		s_first_elem_idx = chunk_list.d_first_elem_idx[chk];
	}

	__syncthreads();

	// Copy values into shared memory.
	__shared__ float s_mem[KD_CHUNKSIZE];
	float3 aabb_min, aabb_max; 

	// Manual unrolling since automatic did not work.
	float v1, v2;
	
	if (num_elems_pts == 1)
	{
		// Use second shared buffer to avoid rereading.
		__shared__ float s_mem2[KD_CHUNKSIZE];
		
		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].x;
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].x;
		s_mem[threadIdx.x] = fminf(v1, v2);
		if (threadIdx.x >= s_num_elems)
			v1 = -M_INFINITY;
		if (threadIdx.x + blockDim.x >= s_num_elems)
			v2 = -M_INFINITY;
		s_mem2[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads();
		aabb_min.x = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());
		aabb_max.x = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem2, op_maximum<float>()); 	
		
		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].y; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].y; 
		s_mem[threadIdx.x] = fminf(v1, v2);
		if (threadIdx.x >= s_num_elems)
			v1 = -M_INFINITY; 
		if (threadIdx.x + blockDim.x >= s_num_elems)
			v2 = -M_INFINITY;
		s_mem2[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads(); 
		aabb_min.y = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());
		aabb_max.y = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem2, op_maximum<float>()); 

		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].z; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].z;
		s_mem[threadIdx.x] = fminf(v1, v2);
		if (threadIdx.x >= s_num_elems)
			v1 = -M_INFINITY; 
		if (threadIdx.x + blockDim.x >= s_num_elems)
			v2 = -M_INFINITY;
		s_mem2[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads(); 
		aabb_min.z = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());
		aabb_max.z = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem2, op_maximum<float>()); 
	}
	else // num_elems_pts == 2
	{
		// first elem point, min 
		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].x; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].x; 
		s_mem[threadIdx.x] = fminf(v1, v2);
		__syncthreads(); 
		aabb_min.x = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());

		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].y; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].y; 
		s_mem[threadIdx.x] = fminf(v1, v2);
		__syncthreads(); 
		aabb_min.y = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());

		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].z; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].z; 
		s_mem[threadIdx.x] = fminf(v1, v2);
		__syncthreads(); 
		aabb_min.z = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_minimum<float>>(s_mem, op_minimum<float>());

		// second elem point, max 
		v1 = -M_INFINITY; v2 = -M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].x; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].x; 
		s_mem[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads(); 
		aabb_max.x = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem, op_maximum<float>());

		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].y; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].y; 
		s_mem[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads(); 
		aabb_max.y = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem, op_maximum<float>());

		v1 = M_INFINITY; v2 = M_INFINITY;
		if (threadIdx.x < s_num_elems)
			v1 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x].z; 
		if (threadIdx.x + blockDim.x < s_num_elems)
			v2 = node_list.d_elem_point1[s_first_elem_idx + threadIdx.x + blockDim.x].z; 
		s_mem[threadIdx.x] = fmaxf(v1, v2);
		__syncthreads(); 
		aabb_max.z = device_reduce_fast<float, (uint32)(KD_CHUNKSIZE/2), op_maximum<float>>(s_mem, op_maximum<float>());
	}

	if (threadIdx.x == 0)
	{
		chunk_list.d_aabb_min[chk] = make_float4(aabb_min);
		chunk_list.d_aabb_max[chk] = make_float4(aabb_max);
	}
}

__global__ void kernel_can_cutoff_empty_space(c_kd_node_list active_list, uint32 axis, bool bmax,  uint32 *d_out_can_cutoff)
{
	uint32 idx = blockIdx.x * blockDim.x + threadIdx.x;
	
	if (idx < active_list.num_nodes)
	{
		// Check for empty space on given side.
		float4 aabb_min_inherit = active_list.d_aabb_inherit_min[idx];
		float4 aabb_max_inherit = active_list.d_aabb_inherit_max[idx];
		float4 aabb_min_tight =  active_list.d_aabb_tight_min[idx];
		float4 aabb_max_tight =  active_list.d_aabb_tight_max[idx];
		float inherit_min = ((float*)&aabb_min_inherit)[axis];
		float inherit_max = ((float*)&aabb_max_inherit)[axis];
		float tight_min = ((float*)&aabb_min_tight)[axis];
		float tight_max = ((float*)&aabb_max_tight)[axis];

		float total = inherit_max - inherit_min;
		float empty_space_ratio = k_empty_space_ratio;

		uint32 can_cutoff = 0;
		if (!bmax)
		{
			// minimum check 
			float empty = tight_min - inherit_min;
			if (empty > total*empty_space_ratio)
				can_cutoff = 1; 
		}
		else
		{
			// maximum check 
			float empty = inherit_max - tight_max;
			if (empty > total*empty_space_ratio)
				can_cutoff = 1; 
		}
		
		d_out_can_cutoff[idx] = can_cutoff;
	}
}


// ---------------------------------------------------------------------
/*
/// \brief	Marks left and right elements in next list ENA.
/// 		
/// 		It is assumed that active list's elements were duplicated in the following way: The
/// 		ENA was copied to the first \c lstActive.nextFreePos elements and to the second \c
/// 		lstActive.nextFreePos elements.  

/// \tparam	numElementPoints	The number of points per element determines the type of the
/// 							elements. For one point, it's a simple point. For two points, we
/// 							assume a bounding box. 
///
/// \param	lstActive			The active list. 
/// \param	lstChunks			The chunk list constructed for the active list. 
/// \param [in]		d_randoms	Uniform random numbers used to avoid endless split loops if
/// 							several elements lie within a splitting plane. There should be a
/// 							random number for each thread, i.e. for each element processed. 
/// \param [out]	d_outValid	Binary 0/1 array ordered as the ENA of the next list, i.e. the
/// 							left child valid flags are in the first half and the right child
/// 							valid flags are in the right half respectively.  
*/ 
// ---------------------------------------------------------------------
template <uint32 num_elem_pts>
__global__ void kernel_mark_left_right_elems(c_kd_node_list active_list, 
											c_kd_chunk_list chunks_list, 
											float *d_randoms, 
											uint32 *d_out_valid)
{
	uint32 chunk = CUDA_GRID2DINDEX;
	uint32 idx = threadIdx.x;
	
	__shared__ uint32 s_num_elems_chunk;
	__shared__ uint32 s_idx_node; 
	__shared__ uint32 s_idx_first_elem;
	__shared__ uint32 s_split_axis;
	__shared__ float s_split_pos;

	if (threadIdx.x == 0)
	{
		s_num_elems_chunk = chunks_list.d_num_elems[chunk];
		s_idx_node = chunks_list.d_node_idx[chunk];
		s_idx_first_elem = chunks_list.d_first_elem_idx[chunk];
		s_split_axis = active_list.d_split_axis[s_idx_node];
		s_split_pos = active_list.d_split_pos[s_idx_node];
	}

	__syncthreads();

	if (idx < s_num_elems_chunk)
	{
		uint32 idx_tna = s_idx_first_elem + idx; 
		uint32 tid = blockDim.x * CUDA_GRID2DINDEX + threadIdx.x;
		bool is_left = false; 
		bool is_right = false; 

		if (num_elem_pts == 2)
		{
			// Get bounds
			float bounds_min = ((float*)&active_list.d_elem_point1[idx_tna])[s_split_axis];
			float bounds_max = ((float*)&active_list.d_elem_point2[idx_tna])[s_split_axis];
			
			// Check on which sides the triangle is. It might be on both sides!
			if (d_randoms[tid] < 0.5f)
			{
				is_left = bounds_min < s_split_pos || (bounds_min == s_split_pos && bounds_min == bounds_max);	
				is_right = s_split_pos < bounds_max; 
			}
			else 
			{
				is_left = bounds_min < s_split_pos;
				is_right = s_split_pos < bounds_max || (bounds_min == s_split_pos && bounds_min == bounds_max); 
			}
		}
		else 
		{
			float val = ((float*)&active_list.d_elem_point1[idx_tna])[s_split_axis];
			// Cannot use the same criterion (i.e. < and <= or <= and <) for all points.
			// Else we would have the special case where all points lie in the splitting plane
			// and therefore all points land on a single side. This would result in an endless
			// loop in large node stage!
			if (d_randoms[tid] < 0.5f)
			{
				is_left = val < s_split_pos;
				is_right = s_split_pos <= val;
			}
			else 
			{
				is_left = val <= s_split_pos;
				is_right = s_split_pos < val;
			}
		}
		
		// left 
		d_out_valid[s_idx_first_elem + idx] = (is_left ? 1 : 0);
		d_out_valid[active_list.next_free_pos + s_idx_first_elem + idx] = (is_right ? 1 : 0);
	}
	
	
}

// ---------------------------------------------------------------------
/*
/// \brief	Performs empty space cutting. 
///
///			For all nodes of the active list, for which empty space cutting on the given side
///			of their AABBs can be performed, the empty space is cut off. This is done by
///			generating new empty and non-empty nodes and inserting them into the final node list.
///			Furthermore the active list is updated to contain the new non-empty nodes. 

/// \param	lstActive					The active node list. Contains the nodes that are to be
/// 									subdivided. When empty space is cut off for some node, its
///										AABB and node level are updated accordingly.
/// \param	lstFinal					The final node list. Will be updated with the generated
///										empty and non-empty nodes.
/// \param	axis						The axis to check. 
/// \param	bMax						Whether to check maximum or minimum sides. 
/// \param [in]		d_canCutOff			Binary 0/1 array. Contains 1 for nodes where empty space
/// 									can be cut off. Generated by
/// 									kernel_CanCutOffEmptySpace(). 
/// \param [in]		d_cutOffsets		Cut offsets. This should be the result of a scan of \a
/// 									d_canCutOff. It will indicate the offset to use for
/// 									writing the generated empty and non-empty child nodes
/// 									into the final node list. 
/// \param	numCuts						Number of cuts. Can be obtained by reduction on \a
/// 									d_canCutOff. 
/// \param [in,out]	d_ioFinalListIndex	Will contain updated final node list indices for the
///										current active list nodes. That is, the generated non-empty
///										node for the i-th active list node can be found at the index
///										\a d_ioFinalListIndex[i].
*/ 
// ---------------------------------------------------------------------
__global__ void kernel_empty_space_cutting(c_kd_node_list active_list, c_kd_final_node_list final_list, uint32 axis, bool bmax,
										uint32 *d_can_cutoff, uint32 *d_cut_offsets, uint32 num_cuts,
										uint32* d_final_list_idx)
{
	uint idx = blockIdx.x * blockDim.x + threadIdx.x;
	
	if (idx < active_list.num_nodes && d_can_cutoff[idx])
	{
		float3 aabb_min_inherit = make_float3(active_list.d_aabb_inherit_min[idx]);
		float3 aabb_max_inherit = make_float3(active_list.d_aabb_inherit_max[idx]);
		float3 aabb_min_tight =  make_float3(active_list.d_aabb_tight_min[idx]);
		float3 aabb_max_tight =  make_float3(active_list.d_aabb_tight_max[idx]);
		
		float aabb_min_child[3] = {((float*)&aabb_min_inherit)[0], ((float*)&aabb_min_inherit)[1], ((float*)&aabb_min_inherit)[2]};
		float aabb_max_child[3] = {((float*)&aabb_max_inherit)[0], ((float*)&aabb_max_inherit)[1], ((float*)&aabb_max_inherit)[2]};		

		uint32 node_level_parent = active_list.d_node_level[idx];

		// Compute indices for left and right node in final node list.
		uint32 cut_offset = d_can_cutoff[idx];
		uint32 idx_parent = d_final_list_idx[idx];
		uint32 idx_left = final_list.num_nodes + cut_offset; 
		uint32 idx_right = final_list.num_nodes + num_cuts + cut_offset;
		float split_pos = (bmax ? ((float*)&aabb_max_tight)[axis] : ((float*)&aabb_min_tight)[axis]);

		if (!bmax)
		{
			// Below (left) is the empty node.
			aabb_max_child[axis] = split_pos;
			device_create_empty_leaf(final_list, idx_left, aabb_min_child, aabb_max_child, node_level_parent+1);
			
			// Above (right) is the tighter node.
			aabb_min_child[axis] = aabb_max_child[axis];
			aabb_max_child[axis] = ((float*)&aabb_max_inherit)[axis];
			device_create_final_node_copy(final_list, idx_parent, idx_right, aabb_min_child, aabb_max_child, node_level_parent+1);

			// Update active list node to describe the above node. Change inherited to be tighter.
			active_list.d_aabb_inherit_min[idx] = make_float4(aabb_min_child[0], aabb_min_child[1], aabb_min_child[2], 0.0f);
			active_list.d_aabb_inherit_max[idx] = make_float4(aabb_max_child[0], aabb_max_child[1], aabb_max_child[2], 0.0f);
			active_list.d_node_level[idx] = node_level_parent+1; 
		}
		else 
		{
			// Below (left) is the tighter node.
			aabb_max_child[axis] = split_pos;
			device_create_final_node_copy(final_list, idx_parent, idx_left, aabb_min_child, aabb_max_child, node_level_parent+1);

			// Update active list node to describe the above node. Change inherited to be tighter.
			active_list.d_aabb_inherit_min[idx] = make_float4(aabb_min_child[0], aabb_min_child[1], aabb_min_child[2], 0.0f);
			active_list.d_aabb_inherit_max[idx] = make_float4(aabb_max_child[0], aabb_max_child[1], aabb_max_child[2], 0.0f);
			active_list.d_node_level[idx] = node_level_parent+1; 

			aabb_min_child[axis] = aabb_max_child[axis];
			aabb_max_child[axis] = ((float*)&aabb_max_inherit)[axis];
			device_create_empty_leaf(final_list, idx_right, aabb_min_child, aabb_max_child, node_level_parent+1);
		}
		
		// Write split information to original node in final node list.
		final_list.d_split_axis[idx_parent] = axis; 
		final_list.d_split_pos[idx_parent] = split_pos;
		final_list.d_child_left[idx_parent] = idx_left;
		final_list.d_child_right[idx_parent] = idx_right;

		// Update final list index to point to the tighter node.
		d_final_list_idx[idx] = (bmax ? idx_left : idx_right);
	}
}

// ---------------------------------------------------------------------
/*
/// \brief	Splits large nodes in active list into smaller nodes.
/// 		
/// 		The resulting nodes are put in the next list, starting from index 0. Left (below)
/// 		nodes are written at the same indices as in active list. Right (above) nodes are
/// 		offsetted by the number of active list nodes.  
*/ 
// ---------------------------------------------------------------------
__global__ void kernel_split_large_nodes(const c_kd_node_list active_list, kd_node_list_aabb next_list)
{
	uint32 idx = blockIdx.x * blockDim.x + threadIdx.x;

	if (idx < active_list.num_nodes)
	{
		// Empty space cutting was performed. Therefore our inherited bounds can be
		// used directly as basis.

		float3 aabb_min_inherit = make_float3(active_list.d_aabb_inherit_min[idx]);
		float3 aabb_max_inherit = make_float3(active_list.d_aabb_inherit_max[idx]);
		
		// Find longest axis of new bounds.
		uint32 longest = 0;
		if (aabb_max_inherit.y - aabb_min_inherit.y > aabb_max_inherit.x - aabb_min_inherit.x && 
			aabb_max_inherit.y - aabb_min_inherit.y > aabb_max_inherit.z- aabb_min_inherit.z)
			longest = 1; 
		else if (aabb_max_inherit.z - aabb_min_inherit.z > aabb_max_inherit.x - aabb_min_inherit.x && 
				aabb_max_inherit.z - aabb_min_inherit.z > aabb_max_inherit.y - aabb_min_inherit.y) 
				longest = 2; 

		// Split position 
		float split_pos = ((float*)&aabb_min_inherit)[longest] + 0.5f * ( ((float*)&aabb_max_inherit)[longest] - ((float*)&aabb_min_inherit)[longest] );
		
		// Store split information
		active_list.d_split_axis[idx] = longest;
		active_list.d_split_pos[idx] = split_pos;

		uint32 old_level = active_list.d_node_level[idx];
		
		// Add the two children for spatial median split.
		float aabb_min_child[3] = {aabb_min_inherit.x, aabb_min_inherit.y, aabb_min_inherit.z};
		float aabb_max_child[3] = {aabb_max_inherit.x, aabb_max_inherit.y, aabb_max_inherit.z}; 
		
		// Below node 
		uint32 idx_write = idx;
		aabb_max_child[longest] = split_pos;
		device_create_child(next_list, next_list.num_nodes + idx_write, aabb_min_child, aabb_max_child, old_level+1);
		aabb_max_child[longest] = ((float*)&aabb_max_inherit)[longest];
		
		active_list.d_child_left[idx] = idx_write;
		// Set first index to same as parent node.
		next_list.d_first_elem_idx[idx_write] = active_list.d_first_elem_idx[idx];
		
		// Above node 
		idx_write = active_list.num_nodes + idx;
		aabb_min_child[longest] = split_pos;
		device_create_child(next_list, next_list.num_nodes + idx_write, aabb_min_child, aabb_max_child, old_level+1);

		active_list.d_child_right[idx] = idx_write;
		// Set first index to offsetted parent node index.
		next_list.d_first_elem_idx[idx_write] = active_list.next_free_pos + active_list.d_first_elem_idx[idx];
 	}
}

//////////////////////////////////////////////////////////////////////////

extern "C"
void init_kd_kernels()
{
	// Find out how many thread blocks (grid dimension) we can use on the current device.
	int cur_device;
	cuda_safe_call_no_sync(cudaGetDevice(&cur_device));
	cuda_safe_call_no_sync(cudaGetDeviceProperties(&device_props, cur_device));
}

extern "C++"
template <uint32 num_elem_pts> 
void kernel_wrapper_gen_chunk_aabb(const c_kd_node_list& active_list, c_kd_chunk_list& chunks_list)
{
	// Note that we use half the chunk size here. This is a reduction optimization.
	dim3 block_size = dim3(KD_CHUNKSIZE/2, 1, 1);
	// Avoid the maximum grid size by using two dimensions.
	dim3 grid_size = CUDA_MAKEGRID2D(chunks_list.num_chunks, device_props.maxGridSize[0]);
	
	kernel_gen_chunk_aabb<num_elem_pts><<<grid_size, block_size>>>(active_list, chunks_list);
}

extern "C"
void kernel_wrappper_empty_space_cutting(c_kd_node_list& active_list, 
										c_kd_final_node_list& final_list, 
										float empty_space_ratio, 
										uint32 *d_io_final_list_idx)
{
	dim3 block_size = dim3(256, 1, 1);
	dim3 grid_size = dim3(CUDA_DIVUP(active_list.num_nodes, block_size.x), 1, 1);
	
	c_cuda_memory<uint32> d_can_cutoff(active_list.num_nodes);
	c_cuda_memory<uint32> d_cut_offsets(active_list.num_nodes);

	// Set empty space ratio
	cuda_safe_call_no_sync(cudaMemcpyToSymbol("k_empty_space_ratio", &empty_space_ratio, sizeof(float)));
	
	for (uint32 is_max = 0; is_max < 2; ++is_max)
	{
		for (uint32 axis = 0; axis < 3; ++axis)
		{
			bool bmax = (is_max == 1);
			kernel_can_cutoff_empty_space<<<grid_size, block_size>>>(active_list, axis, bmax, d_can_cutoff.get_writable_buf_ptr());
			CUDA_CHECKERROR; 

			cuda_scan(d_can_cutoff.get_buf_ptr(), active_list.num_nodes, false, d_cut_offsets.get_writable_buf_ptr());
			
			// Get number of cuts by reduction.
			uint32 num_cuts;
			cuda_reduce_add(num_cuts, (uint32*)d_can_cutoff.get_buf_ptr(), active_list.num_nodes, (uint32)0);
			
			if (num_cuts > 0)
			{
				// Verify we have enough space.
				if (final_list.max_nodes < final_list.num_nodes + 2*num_cuts)
					final_list.resize_node_data(final_list.num_nodes + 2*num_cuts);

				// Perform cut and generate new final list nodes and update active list nodes.
				kernel_empty_space_cutting<<<grid_size, block_size>>>(active_list, final_list, axis, bmax, d_can_cutoff.get_writable_buf_ptr(), d_cut_offsets.get_writable_buf_ptr(), num_cuts, d_io_final_list_idx);
				CUDA_CHECKERROR;
				
				// Update final list node count. It increases by 2*numCuts since we both had to create
				// the empty cut-off node and the tighter node.
				final_list.num_nodes += 2*num_cuts;
			}
		}
	} 
}

extern "C"
void kernel_wrapper_split_large_nodes(const c_kd_node_list& active_list, c_kd_node_list& next_list)
{
	dim3 block_size = dim3(256, 1, 1);
	dim3 grid_size = dim3(CUDA_DIVUP(active_list.num_nodes, block_size.x), 1, 1);
	
	// Convert next list to internal representation.
	kd_node_list_aabb next_list_in(next_list);
	
	kernel_split_large_nodes<<<grid_size, block_size>>>(active_list, next_list_in);
	CUDA_CHECKERROR;
}

extern "C++"
template <uint32 num_elem_pts>
void kernel_wrapper_mark_left_right_elems(const c_kd_node_list& active_list, const c_kd_chunk_list& chunks_list)
{
	dim3 block_size = dim3(KD_CHUNKSIZE, 1, 1);
	dim3 grid_size = CUDA_MAKEGRID2D(chunks_list.num_chunks, device_props.maxGridSize[0]);

	// Build random number array.
	c_cuda_rng& rng = c_cuda_rng::get_instance();
	uint32 num_rands = rng.get_aligned_cnt(chunks_list.num_chunks*KD_CHUNKSIZE);
	c_cuda_memory<float> d_randoms(num_rands);
	rng.seed(rand());
	cuda_safe_call_no_sync(rng.gen_rand(d_randoms.get_writable_buf_ptr(), num_rands));
	
	
	
	
	
}


extern "C++" template void kernel_wrapper_gen_chunk_aabb<1>(const c_kd_node_list& active_list, c_kd_chunk_list& chunks_list);
extern "C++" template void kernel_wrapper_gen_chunk_aabb<2>(const c_kd_node_list& active_list, c_kd_chunk_list& chunks_list);

extern "C++" template void kernel_wrapper_mark_left_right_elems<1>(const c_kd_node_list& active_list, const c_kd_chunk_list& chunks_list);
extern "C++" template void kernel_wrapper_mark_left_right_elems<2>(const c_kd_node_list& active_list, const c_kd_chunk_list& chunks_list);